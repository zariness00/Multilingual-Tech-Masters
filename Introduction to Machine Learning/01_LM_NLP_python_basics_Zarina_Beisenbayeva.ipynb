{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/liadmagen/NLP-Course/blob/master/exercises_notebooks/01_LM_NLP_python_basics.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9k8FCi_UAtCN"
   },
   "source": [
    "# Python Basics\n",
    "\n",
    "In this exercise, we'll explore python NLP capabilities with the help of the package `nltk`.\n",
    "\n",
    "By the end of the exercise, you will:\n",
    "* be introduced to the nltk package and its functionality\n",
    "* understand the basics of text analysis, and know how to approach this unstructured data.\n",
    "* Understand the terms 'n-gram' & 'collocation'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iF7evQxY31Kr"
   },
   "source": [
    "We are going to use the package `NLTK` - 'Natural Language Toolkit' (https://www.nltk.org/).\n",
    "\n",
    "NLTK is a great package for research and for learning. However, it isn't recommended for production use and for real-world applications, as it isn't fast enough and therefore doesn't scale."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1OqX9GeN216c"
   },
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "wS2o9QMaUmzC"
   },
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3gBd4ZgXzAmP",
    "outputId": "42744652-aa5d-4e6e-8174-ac06d9585591"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading collection 'book'\n",
      "[nltk_data]    | \n",
      "[nltk_data]    | Downloading package abc to\n",
      "[nltk_data]    |     C:\\Users\\exol1\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package abc is already up-to-date!\n",
      "[nltk_data]    | Downloading package brown to\n",
      "[nltk_data]    |     C:\\Users\\exol1\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package brown is already up-to-date!\n",
      "[nltk_data]    | Downloading package chat80 to\n",
      "[nltk_data]    |     C:\\Users\\exol1\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package chat80 is already up-to-date!\n",
      "[nltk_data]    | Downloading package cmudict to\n",
      "[nltk_data]    |     C:\\Users\\exol1\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package cmudict is already up-to-date!\n",
      "[nltk_data]    | Downloading package conll2000 to\n",
      "[nltk_data]    |     C:\\Users\\exol1\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package conll2000 is already up-to-date!\n",
      "[nltk_data]    | Downloading package conll2002 to\n",
      "[nltk_data]    |     C:\\Users\\exol1\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package conll2002 is already up-to-date!\n",
      "[nltk_data]    | Downloading package dependency_treebank to\n",
      "[nltk_data]    |     C:\\Users\\exol1\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package dependency_treebank is already up-to-date!\n",
      "[nltk_data]    | Downloading package genesis to\n",
      "[nltk_data]    |     C:\\Users\\exol1\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package genesis is already up-to-date!\n",
      "[nltk_data]    | Downloading package gutenberg to\n",
      "[nltk_data]    |     C:\\Users\\exol1\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package gutenberg is already up-to-date!\n",
      "[nltk_data]    | Downloading package ieer to\n",
      "[nltk_data]    |     C:\\Users\\exol1\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package ieer is already up-to-date!\n",
      "[nltk_data]    | Downloading package inaugural to\n",
      "[nltk_data]    |     C:\\Users\\exol1\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package inaugural is already up-to-date!\n",
      "[nltk_data]    | Downloading package movie_reviews to\n",
      "[nltk_data]    |     C:\\Users\\exol1\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package movie_reviews is already up-to-date!\n",
      "[nltk_data]    | Downloading package nps_chat to\n",
      "[nltk_data]    |     C:\\Users\\exol1\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package nps_chat is already up-to-date!\n",
      "[nltk_data]    | Downloading package names to\n",
      "[nltk_data]    |     C:\\Users\\exol1\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package names is already up-to-date!\n",
      "[nltk_data]    | Downloading package ppattach to\n",
      "[nltk_data]    |     C:\\Users\\exol1\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package ppattach is already up-to-date!\n",
      "[nltk_data]    | Downloading package reuters to\n",
      "[nltk_data]    |     C:\\Users\\exol1\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package reuters is already up-to-date!\n",
      "[nltk_data]    | Downloading package senseval to\n",
      "[nltk_data]    |     C:\\Users\\exol1\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package senseval is already up-to-date!\n",
      "[nltk_data]    | Downloading package state_union to\n",
      "[nltk_data]    |     C:\\Users\\exol1\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package state_union is already up-to-date!\n",
      "[nltk_data]    | Downloading package stopwords to\n",
      "[nltk_data]    |     C:\\Users\\exol1\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package stopwords is already up-to-date!\n",
      "[nltk_data]    | Downloading package swadesh to\n",
      "[nltk_data]    |     C:\\Users\\exol1\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package swadesh is already up-to-date!\n",
      "[nltk_data]    | Downloading package timit to\n",
      "[nltk_data]    |     C:\\Users\\exol1\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package timit is already up-to-date!\n",
      "[nltk_data]    | Downloading package treebank to\n",
      "[nltk_data]    |     C:\\Users\\exol1\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package treebank is already up-to-date!\n",
      "[nltk_data]    | Downloading package toolbox to\n",
      "[nltk_data]    |     C:\\Users\\exol1\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package toolbox is already up-to-date!\n",
      "[nltk_data]    | Downloading package udhr to\n",
      "[nltk_data]    |     C:\\Users\\exol1\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package udhr is already up-to-date!\n",
      "[nltk_data]    | Downloading package udhr2 to\n",
      "[nltk_data]    |     C:\\Users\\exol1\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package udhr2 is already up-to-date!\n",
      "[nltk_data]    | Downloading package unicode_samples to\n",
      "[nltk_data]    |     C:\\Users\\exol1\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package unicode_samples is already up-to-date!\n",
      "[nltk_data]    | Downloading package webtext to\n",
      "[nltk_data]    |     C:\\Users\\exol1\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package webtext is already up-to-date!\n",
      "[nltk_data]    | Downloading package wordnet to\n",
      "[nltk_data]    |     C:\\Users\\exol1\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package wordnet is already up-to-date!\n",
      "[nltk_data]    | Downloading package wordnet_ic to\n",
      "[nltk_data]    |     C:\\Users\\exol1\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package wordnet_ic is already up-to-date!\n",
      "[nltk_data]    | Downloading package words to\n",
      "[nltk_data]    |     C:\\Users\\exol1\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package words is already up-to-date!\n",
      "[nltk_data]    | Downloading package maxent_treebank_pos_tagger to\n",
      "[nltk_data]    |     C:\\Users\\exol1\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package maxent_treebank_pos_tagger is already up-\n",
      "[nltk_data]    |       to-date!\n",
      "[nltk_data]    | Downloading package maxent_ne_chunker to\n",
      "[nltk_data]    |     C:\\Users\\exol1\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package maxent_ne_chunker is already up-to-date!\n",
      "[nltk_data]    | Downloading package universal_tagset to\n",
      "[nltk_data]    |     C:\\Users\\exol1\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package universal_tagset is already up-to-date!\n",
      "[nltk_data]    | Downloading package punkt to\n",
      "[nltk_data]    |     C:\\Users\\exol1\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package punkt is already up-to-date!\n",
      "[nltk_data]    | Downloading package book_grammars to\n",
      "[nltk_data]    |     C:\\Users\\exol1\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package book_grammars is already up-to-date!\n",
      "[nltk_data]    | Downloading package city_database to\n",
      "[nltk_data]    |     C:\\Users\\exol1\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package city_database is already up-to-date!\n",
      "[nltk_data]    | Downloading package tagsets to\n",
      "[nltk_data]    |     C:\\Users\\exol1\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package tagsets is already up-to-date!\n",
      "[nltk_data]    | Downloading package panlex_swadesh to\n",
      "[nltk_data]    |     C:\\Users\\exol1\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package panlex_swadesh is already up-to-date!\n",
      "[nltk_data]    | Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]    |     C:\\Users\\exol1\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package averaged_perceptron_tagger is already up-\n",
      "[nltk_data]    |       to-date!\n",
      "[nltk_data]    | \n",
      "[nltk_data]  Done downloading collection book\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('book')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "LRzGNH63xcwQ",
    "outputId": "64fdb9b6-c709-4cc3-d797-3b6524f7bb32"
   },
   "outputs": [],
   "source": [
    "from nltk.book import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "J_dcD1AYhBez"
   },
   "source": [
    "# Exploratory Data Analysis (EDA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FlR3eNPu3EWS"
   },
   "source": [
    "## A Closer Look at Python: Texts as Lists of Words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ul9GAlK63sMj"
   },
   "source": [
    "We will use the great book 'Moby Dick' by Herman Melville, as our learning experiment playground.\n",
    "\n",
    "The book is already tokenized and stored as a list of these tokens, under a variable with the excellent, well expressed name - `text1` (please do yourself - and me - a favor and name your variables in a more meaningful manner than that...).\n",
    "\n",
    "We start - as you should always do - with exploring and looking at our dataset.\n",
    "\n",
    "Let's peek at the first 100 words:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jjIvrcow3Pjc",
    "outputId": "f563c846-a188-46a8-9bb8-17496c152249"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['[',\n",
       " 'Moby',\n",
       " 'Dick',\n",
       " 'by',\n",
       " 'Herman',\n",
       " 'Melville',\n",
       " '1851',\n",
       " ']',\n",
       " 'ETYMOLOGY',\n",
       " '.',\n",
       " '(',\n",
       " 'Supplied',\n",
       " 'by',\n",
       " 'a',\n",
       " 'Late',\n",
       " 'Consumptive',\n",
       " 'Usher',\n",
       " 'to',\n",
       " 'a',\n",
       " 'Grammar',\n",
       " 'School',\n",
       " ')',\n",
       " 'The',\n",
       " 'pale',\n",
       " 'Usher',\n",
       " '--',\n",
       " 'threadbare',\n",
       " 'in',\n",
       " 'coat',\n",
       " ',',\n",
       " 'heart',\n",
       " ',',\n",
       " 'body',\n",
       " ',',\n",
       " 'and',\n",
       " 'brain',\n",
       " ';',\n",
       " 'I',\n",
       " 'see',\n",
       " 'him',\n",
       " 'now',\n",
       " '.',\n",
       " 'He',\n",
       " 'was',\n",
       " 'ever',\n",
       " 'dusting',\n",
       " 'his',\n",
       " 'old',\n",
       " 'lexicons',\n",
       " 'and',\n",
       " 'grammars',\n",
       " ',',\n",
       " 'with',\n",
       " 'a',\n",
       " 'queer',\n",
       " 'handkerchief',\n",
       " ',',\n",
       " 'mockingly',\n",
       " 'embellished',\n",
       " 'with',\n",
       " 'all',\n",
       " 'the',\n",
       " 'gay',\n",
       " 'flags',\n",
       " 'of',\n",
       " 'all',\n",
       " 'the',\n",
       " 'known',\n",
       " 'nations',\n",
       " 'of',\n",
       " 'the',\n",
       " 'world',\n",
       " '.',\n",
       " 'He',\n",
       " 'loved',\n",
       " 'to',\n",
       " 'dust',\n",
       " 'his',\n",
       " 'old',\n",
       " 'grammars',\n",
       " ';',\n",
       " 'it',\n",
       " 'somehow',\n",
       " 'mildly',\n",
       " 'reminded',\n",
       " 'him',\n",
       " 'of',\n",
       " 'his',\n",
       " 'mortality',\n",
       " '.',\n",
       " '\"',\n",
       " 'While',\n",
       " 'you',\n",
       " 'take',\n",
       " 'in',\n",
       " 'hand',\n",
       " 'to',\n",
       " 'school',\n",
       " 'others',\n",
       " ',']"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#changed the variable name\n",
    "mody_dick_book1 = text1\n",
    "moby_dick_book1[:100]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gD5ADOnC48oM"
   },
   "source": [
    "Pay attention that punctuations here are also conisdered as a `token`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uthWKmm153r-"
   },
   "source": [
    "## Exercise #1: Show the last 23 tokens in the book:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "SvWRtYl-3Ubi"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The last sentence of the book is: \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['It',\n",
       " 'was',\n",
       " 'the',\n",
       " 'devious',\n",
       " '-',\n",
       " 'cruising',\n",
       " 'Rachel',\n",
       " ',',\n",
       " 'that',\n",
       " 'in',\n",
       " 'her',\n",
       " 'retracing',\n",
       " 'search',\n",
       " 'after',\n",
       " 'her',\n",
       " 'missing',\n",
       " 'children',\n",
       " ',',\n",
       " 'only',\n",
       " 'found',\n",
       " 'another',\n",
       " 'orphan',\n",
       " '.']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### YOUR TURN:\n",
    "### Write a code that shows the last sentence (23 tokens) of the book\n",
    "print(\"The last sentence of the book is: \")\n",
    "moby_dick_book1[-23:]\n",
    "\n",
    "### End"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sC0w4C5T68wF"
   },
   "source": [
    "## Lists vs Sets\n",
    "\n",
    "In python, an ordered set, with repetition, is defined as a `List`, and is defied by sqaured braces [].\n",
    "\n",
    "An unordered set, where repetitions are *discarded*, is defined with curly braces: {}.\n",
    "\n",
    "When converting a list into a set, we can get the **vocabulary** of the corpus, the *unique* words that the dataset is constructed of:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "MsUBYcwl6Cdc",
    "outputId": "0c9aed26-6ea6-47ce-ef34-31faf4559bcc"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['yawned',\n",
       " 'yawning',\n",
       " 'ye',\n",
       " 'yea',\n",
       " 'year',\n",
       " 'yearly',\n",
       " 'years',\n",
       " 'yeast',\n",
       " 'yell',\n",
       " 'yelled',\n",
       " 'yelling',\n",
       " 'yellow',\n",
       " 'yellowish',\n",
       " 'yells',\n",
       " 'yes',\n",
       " 'yesterday',\n",
       " 'yet',\n",
       " 'yield',\n",
       " 'yielded',\n",
       " 'yielding',\n",
       " 'yields',\n",
       " 'yoke',\n",
       " 'yoked',\n",
       " 'yokes',\n",
       " 'yoking',\n",
       " 'yon',\n",
       " 'yonder',\n",
       " 'yore',\n",
       " 'you',\n",
       " 'young',\n",
       " 'younger',\n",
       " 'youngest',\n",
       " 'youngish',\n",
       " 'your',\n",
       " 'yours',\n",
       " 'yourselbs',\n",
       " 'yourself',\n",
       " 'yourselves',\n",
       " 'youth',\n",
       " 'youthful',\n",
       " 'zag',\n",
       " 'zay',\n",
       " 'zeal',\n",
       " 'zephyr',\n",
       " 'zig',\n",
       " 'zodiac',\n",
       " 'zone',\n",
       " 'zoned',\n",
       " 'zones',\n",
       " 'zoology']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab = set(moby_dick_book1)\n",
    "\n",
    "# We can't get the 'last 25 words', since there is no order...\n",
    "# But we can convert it into a list first, and even sort it\n",
    "list(sorted(vocab))[-50:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LkvijxKv8nqm"
   },
   "source": [
    "## Exercise #2: Vocabulary Length\n",
    "\n",
    "How many words does our vocabulary contain?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "XJnFJcsG8k2X"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is the size of Moby Dick book's vocabulary 19317\n"
     ]
    }
   ],
   "source": [
    "### YOUR TURN:\n",
    "### Write python code that prints the size of Moby Dick book's vocabulary\n",
    "vocab_size = len(set(moby_dick_book1))\n",
    "print(f\"This is the size of Moby Dick book's vocabulary {vocab_size}\")\n",
    "### End"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QxlwVLXG9BRg"
   },
   "source": [
    "# Text Analysis: Frequency Distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IlpuCzpK9H9J"
   },
   "source": [
    "[nltk](http://www.nltk.org) is a library with many research tools for probabilistic information and dataset exploration.\n",
    "\n",
    "For example, it includes a function, `FreqDist`, that return the probability of the occurance of a word in a text:\n",
    "\n",
    "http://www.nltk.org/api/nltk.html?highlight=freqdist#module-nltk.probability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "id": "z76z5LY19FQi"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "These are top-used words in Moby Dick\n",
      "[(',', 18713), ('the', 13721), ('.', 6862), ('of', 6536), ('and', 6024), ('a', 4569), ('to', 4542), (';', 4072), ('in', 3916), ('that', 2982)]\n",
      "These are the frequences of given words: \n",
      "'with': 1659\n",
      "'Moby': 84\n",
      "'fish': 133\n",
      "'whale': 906\n"
     ]
    }
   ],
   "source": [
    "### YOUR TURN:\n",
    "## 1) Write python function named `get_most_frequent(n: int)` that calculates the frequency of words in text1 and returns the top n common ones (n is given as a parameter).\n",
    "## 2) Write a python function - `get_frequency(words: list[str])` that given a list of words, prints the frequency of each of those words in text1.\n",
    "## 3) Use the functions to print how many times the words 'with', 'Moby', 'fish' and 'whale' appear in the book.\n",
    "## hint: FreqDist is a smart python dictionary that already has methods for these tasks, such as .most_common()\n",
    "\n",
    "from nltk.probability import FreqDist\n",
    "\n",
    "def get_most_frequent(n: int):\n",
    "    fdist = FreqDist(moby_dick_book1)\n",
    "    most_common_words = fdist.most_common(n)\n",
    "    return most_common_words\n",
    "\n",
    "\n",
    "def get_frequency(words):\n",
    "    fdist = FreqDist(moby_dick_book1)\n",
    "    for word in words:\n",
    "        print(f\"'{word}': {fdist[word]}\")\n",
    "\n",
    "\n",
    "\n",
    "top_used_words = get_most_frequent(10)\n",
    "print(f\"These are top-used words in Moby Dick\\n{top_used_words}\")\n",
    "print(\"These are the frequences of given words: \")\n",
    "get_frequency([\"with\", \"Moby\", \"fish\", \"whale\"])\n",
    "### End"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "ttVE805EfZP3"
   },
   "outputs": [],
   "source": [
    "assert get_most_frequent(5) == [(',', 18713), ('the', 13721), ('.', 6862), ('of', 6536), ('and', 6024)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "F39jB_1jAboe"
   },
   "source": [
    ":Some of the common words are actually punctuations and '**stop-words**'. They don't help us much with our text analysis, and therefore can be safely ignored.\n",
    "\n",
    "Luckily, NLTK supplies a list of stop words, and python has the punctuation built in into the string package:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Qne74woEATJo",
    "outputId": "8c9c71f1-2fdb-4f51-833d-195a937e19f4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "\n",
    "print(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8NWLKWf79z94",
    "outputId": "067d44ab-3267-42fa-b054-d73e08e4c72e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "!\"#$%&'()*+,-./:;<=>?@[\\]^_`{|}~\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "\n",
    "print(string.punctuation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "id": "APuonB9pA1T3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is the filtered output: [('whale', 906), ('one', 889), ('like', 624), ('upon', 538), ('man', 508)]\n"
     ]
    }
   ],
   "source": [
    "### Write a function - get_most_frequent_filtered(n: int) - that returns the top\n",
    "### n frequennt words, after filtering out stop words and punctuation.\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.probability import FreqDist\n",
    "import string\n",
    "\n",
    "def get_most_frequent_filtered(n: int):\n",
    "    #setting the program with taking into the account stopwords and punct signs\n",
    "    stop_words = set(stopwords.words(\"english\"))\n",
    "    punct_signs = set(string.punctuation)\n",
    "    \n",
    "    # filtering punct signs with the all() function \n",
    "    filtered_words = [\n",
    "        word for word in moby_dick_book1 \n",
    "        if word.lower() not in stop_words \n",
    "        and not all(char in punct_signs for char in word)\n",
    "    ]\n",
    "    #second option to filter punct signs with the .islanum()same output \n",
    "    #filtered_words = [ \n",
    "       #word for word in moby_dick_book1\n",
    "       #if word.lower() not in stop_words\n",
    "       #and any(char.isalnum() for char in word)\n",
    "    #] \n",
    "    \n",
    "    # calculating the frequency\n",
    "    fdist = FreqDist(filtered_words)\n",
    "    most_common_words = fdist.most_common(n)\n",
    "    \n",
    "    return most_common_words\n",
    "\n",
    "filtered_output = get_most_frequent_filtered(5)\n",
    "print(f\"This is the filtered output: {modified_output}\")\n",
    "###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "id": "Y17Fx8baf68k"
   },
   "outputs": [],
   "source": [
    "assert get_most_frequent_filtered(5) == [('whale', 906), ('one', 889), ('like', 624), ('upon', 538), ('man', 508)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wGCb2GiEEuqM"
   },
   "source": [
    "FreqDist can be used even further. Let's analyse the text by the word length.\n",
    "\n",
    "Using python ['list-comprehension'](https://docs.python.org/3/tutorial/datastructures.html#list-comprehensions) method, we can easily get a list of all the words by their lengths:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "QBYSet5WEtcS",
    "outputId": "57d8c265-b893-4e23-bcbb-f5b087530135"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1,\n",
       " 4,\n",
       " 4,\n",
       " 2,\n",
       " 6,\n",
       " 8,\n",
       " 4,\n",
       " 1,\n",
       " 9,\n",
       " 1,\n",
       " 1,\n",
       " 8,\n",
       " 2,\n",
       " 1,\n",
       " 4,\n",
       " 11,\n",
       " 5,\n",
       " 2,\n",
       " 1,\n",
       " 7,\n",
       " 6,\n",
       " 1,\n",
       " 3,\n",
       " 4,\n",
       " 5,\n",
       " 2,\n",
       " 10,\n",
       " 2,\n",
       " 4,\n",
       " 1]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# For convenience of reading, showing here only the first 30\n",
    "[len(w) for w in moby_dick_book1][:30]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gK9wjxkQbP2I"
   },
   "source": [
    "## Exercise #3 (Advanced): Length Frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "id": "BgHoCYA-FXDa"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word Length Frequencies: \n",
      "Length 1: 47933\n",
      "Length 4: 42345\n",
      "Length 2: 38513\n",
      "Length 6: 17111\n",
      "Length 8: 9966\n",
      "Length 9: 6428\n",
      "Length 11: 1873\n",
      "Length 5: 26597\n",
      "Length 7: 14399\n",
      "Length 3: 50223\n",
      "Length 10: 3528\n",
      "Length 12: 1053\n",
      "Length 13: 567\n",
      "Length 14: 177\n",
      "Length 16: 22\n",
      "Length 15: 70\n",
      "Length 17: 12\n",
      "Length 18: 1\n",
      "Length 20: 1\n",
      "20 Longest Words:\n",
      "uninterpenetratingly\n",
      "characteristically\n",
      "uncompromisedness\n",
      "superstitiousness\n",
      "comprehensiveness\n",
      "indispensableness\n",
      "uncomfortableness\n",
      "subterraneousness\n",
      "preternaturalness\n",
      "cannibalistically\n",
      "circumnavigations\n",
      "irresistibleness\n",
      "supernaturalness\n",
      "circumnavigation\n",
      "Physiognomically\n",
      "indiscriminately\n",
      "CIRCUMNAVIGATION\n",
      "circumnavigating\n",
      "simultaneousness\n",
      "apprehensiveness\n",
      "Total Appearances of 20 Longest Words: 31\n"
     ]
    }
   ],
   "source": [
    "### Write a code to calculate the words lengthes frequency inside `text1`.\n",
    "### Find out what those 20 words are.\n",
    "### How many times do the 20 most lengthiest words appear in the text?\n",
    "\n",
    "from nltk.probability import FreqDist\n",
    "\n",
    "# calculating word length frequencies\n",
    "length_freq = FreqDist([len(w) for w in moby_dick_book1])\n",
    "print(f\"Word Length Frequencies: \")\n",
    "for length, freq in length_freq.items():\n",
    "    print(f\"Length {length}: {freq}\")\n",
    "\n",
    "\n",
    "# finding the 20 longest unique words\n",
    "unique_words = set(moby_dick_book1)  # getting unique words to avoid duplicates\n",
    "longest_words = sorted(unique_words, key=len, reverse=True)[:20]\n",
    "print(\"20 Longest Words:\")\n",
    "for word in longest_words:\n",
    "    print(word)\n",
    "\n",
    "# claculating total appearances of the 20 longest words\n",
    "word_counts = FreqDist(moby_dick_book1)  \n",
    "total_appearances = sum(word_counts[word] for word in longest_words)\n",
    "print(f\"Total Appearances of 20 Longest Words: {total_appearances}\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### End"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Buil9-Z-Emvk"
   },
   "source": [
    "# Text Analysis: n-grams and collocation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fVKEjI4GCXei"
   },
   "source": [
    "As we saw in class, a word might not always also be a `token`. In the case of 'New York', 'ice cream', 'red wine', etc., every word meaning on its own is different than the combined meaning as a phrase.\n",
    "\n",
    "A **collocation** is a sequence of words that occur together unusually often.\n",
    "\n",
    "An `n-gram` is a sequence of a size of 'n' of tokens (i.e. words):\n",
    "\n",
    "* When n=1: it is called **unigram**\n",
    "* When n=2: it is called **bigram**\n",
    "* When n=3: it is called **trigram** ...\n",
    "* When n>3: it is just called an **n-gram** with the size of 4.\n",
    "\n",
    "\n",
    "NLTK has two functions: `bigrams` and `collocations`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "itg3dYQ3DeSG",
    "outputId": "9170ad7d-29d1-4497-f1cd-507287fe4653"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1, 2), (2, 3), (3, 4), (4, 5)]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(bigrams([1,2,3,4,5]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "HosjcKYuEIac",
    "outputId": "76c520b3-887d-4a5b-ada7-660936bbd076"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('[', 'Moby'),\n",
       " ('Moby', 'Dick'),\n",
       " ('Dick', 'by'),\n",
       " ('by', 'Herman'),\n",
       " ('Herman', 'Melville'),\n",
       " ('Melville', '1851'),\n",
       " ('1851', ']'),\n",
       " (']', 'ETYMOLOGY'),\n",
       " ('ETYMOLOGY', '.'),\n",
       " ('.', '('),\n",
       " ('(', 'Supplied'),\n",
       " ('Supplied', 'by'),\n",
       " ('by', 'a'),\n",
       " ('a', 'Late'),\n",
       " ('Late', 'Consumptive'),\n",
       " ('Consumptive', 'Usher'),\n",
       " ('Usher', 'to'),\n",
       " ('to', 'a'),\n",
       " ('a', 'Grammar'),\n",
       " ('Grammar', 'School')]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Bigrams generates bi-grams from the text: every two words would be collected together.\n",
    "list(bigrams(moby_dick_book1))[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "id": "PUqPHOytDfBx"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here are the collocations: \n",
      "Sperm Whale; Moby Dick; White Whale; old man; Captain Ahab; sperm\n",
      "whale; Right Whale; Captain Peleg; New Bedford; Cape Horn; cried Ahab;\n",
      "years ago; lower jaw; never mind; Father Mapple; cried Stubb; chief\n",
      "mate; white whale; ivory leg; one hand\n"
     ]
    }
   ],
   "source": [
    "### Your Turn ###\n",
    "# Write here code that returns and print the collocations in text1\n",
    "print(\"Here are the collocations: \")\n",
    "moby_dick_book1.collocations()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Expected output:\n",
    "# Sperm Whale; Moby Dick; White Whale; old man; Captain Ahab; sperm\n",
    "# whale; Right Whale; Captain Peleg; New Bedford; Cape Horn; cried Ahab;\n",
    "# years ago; lower jaw; never mind; Father Mapple; cried Stubb; chief\n",
    "# mate; white whale; ivory leg; one hand\n",
    "\n",
    "### END ###########"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zko0TVbLHWbl"
   },
   "source": [
    "# Python and NLP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "T2vqtYKYHaPa"
   },
   "source": [
    "Python has many strong capabilities, built in, when it comes to string and text procesing, combined with the list comprehension.\n",
    "\n",
    "Here are some examples of filtering the word list:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "FWs-o3xoHZck",
    "outputId": "a8523975-1ece-49fd-c5fd-87b8cb0ca5ac"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['comfortableness',\n",
       " 'honourableness',\n",
       " 'immutableness',\n",
       " 'indispensableness',\n",
       " 'indomitableness',\n",
       " 'intolerableness',\n",
       " 'palpableness',\n",
       " 'reasonableness',\n",
       " 'uncomfortableness']"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get all the words that ends with 'ableness', sorted:\n",
    "sorted(w for w in set(moby_dick_book1) if w.endswith('ableness'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "I1Dp1G3nHv3N",
    "outputId": "3ed704f5-08ac-4801-e512-ddec6f245dd0"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['camphorated',\n",
       " 'corroborated',\n",
       " 'decorated',\n",
       " 'elaborate',\n",
       " 'elaborately',\n",
       " 'evaporate',\n",
       " 'evaporates',\n",
       " 'incorporate',\n",
       " 'incorporated']"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get all the words that contains 'orate', sorted:\n",
    "sorted(term for term in set(moby_dick_book1) if 'orate' in term)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YxVjcOg_H1e8",
    "outputId": "6307efcf-95a9-4847-de80-1de6bb1c9c3b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['3D',\n",
       " 'A',\n",
       " 'Abashed',\n",
       " 'Abednego',\n",
       " 'Abel',\n",
       " 'Abjectus',\n",
       " 'Aboard',\n",
       " 'Abominable',\n",
       " 'About',\n",
       " 'Above',\n",
       " 'Abraham',\n",
       " 'Academy',\n",
       " 'Accessory',\n",
       " 'According',\n",
       " 'Accordingly',\n",
       " 'Accursed',\n",
       " 'Achilles',\n",
       " 'Actium',\n",
       " 'Acushnet',\n",
       " 'Adam',\n",
       " 'Adieu',\n",
       " 'Adios',\n",
       " 'Admiral',\n",
       " 'Admirals',\n",
       " 'Advance',\n",
       " 'Advancement',\n",
       " 'Adventures',\n",
       " 'Adverse',\n",
       " 'Advocate',\n",
       " 'Affected',\n",
       " 'Affidavit',\n",
       " 'Affrighted',\n",
       " 'Afric',\n",
       " 'Africa',\n",
       " 'African',\n",
       " 'Africans',\n",
       " 'Aft',\n",
       " 'After',\n",
       " 'Afterwards',\n",
       " 'Again',\n",
       " 'Against',\n",
       " 'Agassiz',\n",
       " 'Ages',\n",
       " 'Ah',\n",
       " 'Ahab',\n",
       " 'Ahabs',\n",
       " 'Ahasuerus',\n",
       " 'Ahaz',\n",
       " 'Ahoy',\n",
       " 'Ain',\n",
       " 'Air',\n",
       " 'Akin',\n",
       " 'Alabama',\n",
       " 'Aladdin',\n",
       " 'Alarmed',\n",
       " 'Alas',\n",
       " 'Albatross',\n",
       " 'Albemarle',\n",
       " 'Albert',\n",
       " 'Albicore',\n",
       " 'Albino',\n",
       " 'Aldrovandi',\n",
       " 'Aldrovandus',\n",
       " 'Alexander',\n",
       " 'Alexanders',\n",
       " 'Alfred',\n",
       " 'Algerine',\n",
       " 'Algiers',\n",
       " 'Alike',\n",
       " 'Alive',\n",
       " 'All',\n",
       " 'Alleghanian',\n",
       " 'Alleghanies',\n",
       " 'Alley',\n",
       " 'Almanack',\n",
       " 'Almighty',\n",
       " 'Almost',\n",
       " 'Aloft',\n",
       " 'Alone',\n",
       " 'Alps',\n",
       " 'Already',\n",
       " 'Also',\n",
       " 'Am',\n",
       " 'Ambergriese',\n",
       " 'Ambergris',\n",
       " 'Amelia',\n",
       " 'America',\n",
       " 'American',\n",
       " 'Americans',\n",
       " 'Americas',\n",
       " 'Amittai',\n",
       " 'Among',\n",
       " 'Amsterdam',\n",
       " 'An',\n",
       " 'Anacharsis',\n",
       " 'Anak',\n",
       " 'Anatomist',\n",
       " 'And',\n",
       " 'Andes',\n",
       " 'Andrew',\n",
       " 'Andromeda',\n",
       " 'Angel',\n",
       " 'Angelo',\n",
       " 'Angels',\n",
       " 'Animated',\n",
       " 'Annawon',\n",
       " 'Anne',\n",
       " 'Anno',\n",
       " 'Anomalous',\n",
       " 'Another',\n",
       " 'Answer',\n",
       " 'Antarctic',\n",
       " 'Antilles',\n",
       " 'Antiochus',\n",
       " 'Antony',\n",
       " 'Antwerp',\n",
       " 'Anvil',\n",
       " 'Any',\n",
       " 'Anyhow',\n",
       " 'Anything',\n",
       " 'Anyway',\n",
       " 'Apollo',\n",
       " 'Apoplexy',\n",
       " 'Applied',\n",
       " 'Apply',\n",
       " 'April',\n",
       " 'Aquarius',\n",
       " 'Arch',\n",
       " 'Archbishop',\n",
       " 'Arched',\n",
       " 'Archer',\n",
       " 'Archipelagoes',\n",
       " 'Archy',\n",
       " 'Arctic',\n",
       " 'Are',\n",
       " 'Arethusa',\n",
       " 'Argo',\n",
       " 'Aries',\n",
       " 'Arion',\n",
       " 'Aristotle',\n",
       " 'Ark',\n",
       " 'Arkansas',\n",
       " 'Arkite',\n",
       " 'Arm',\n",
       " 'Armada',\n",
       " 'Arnold',\n",
       " 'Aroostook',\n",
       " 'Around',\n",
       " 'Arrayed',\n",
       " 'Arrived',\n",
       " 'Arsacidean',\n",
       " 'Arsacides',\n",
       " 'Art',\n",
       " 'Artedi',\n",
       " 'Arter',\n",
       " 'Articles',\n",
       " 'As',\n",
       " 'Asa',\n",
       " 'Ashantee',\n",
       " 'Ashore',\n",
       " 'Asia',\n",
       " 'Asiatic',\n",
       " 'Asiatics',\n",
       " 'Aside',\n",
       " 'Asphaltites',\n",
       " 'Assaulted',\n",
       " 'Assume',\n",
       " 'Assuming',\n",
       " 'Assuredly',\n",
       " 'Assyrian',\n",
       " 'Astern',\n",
       " 'Astir',\n",
       " 'Astronomy',\n",
       " 'At',\n",
       " 'Atlantic',\n",
       " 'Atlantics',\n",
       " 'Attached',\n",
       " 'Attend',\n",
       " 'August',\n",
       " 'Aunt',\n",
       " 'Australia',\n",
       " 'Australian',\n",
       " 'Austrian',\n",
       " 'Author',\n",
       " 'Authors',\n",
       " 'Auto',\n",
       " 'Availing',\n",
       " 'Avast',\n",
       " 'Avatar',\n",
       " 'Aware',\n",
       " 'Away',\n",
       " 'Awful',\n",
       " 'Ay',\n",
       " 'Aye',\n",
       " 'Azores',\n",
       " 'Babel',\n",
       " 'Babylon',\n",
       " 'Babylonian',\n",
       " 'Bachelor',\n",
       " 'Back',\n",
       " 'Backs',\n",
       " 'Bad',\n",
       " 'Baden',\n",
       " 'Bag',\n",
       " 'Balaene',\n",
       " 'Baliene',\n",
       " 'Baling',\n",
       " 'Bally',\n",
       " 'Baltic',\n",
       " 'Baltimore',\n",
       " 'Bamboo',\n",
       " 'Bang',\n",
       " 'Banks',\n",
       " 'Barbary',\n",
       " 'Bare',\n",
       " 'Bargain',\n",
       " 'Baron',\n",
       " 'Barrens',\n",
       " 'Bartholomew',\n",
       " 'Base',\n",
       " 'Bashaw',\n",
       " 'Bashee',\n",
       " 'Basilosaurus',\n",
       " 'Bastille',\n",
       " 'Battering',\n",
       " 'Battery',\n",
       " 'Bay',\n",
       " 'Bays',\n",
       " 'Be',\n",
       " 'Beach',\n",
       " 'Beale',\n",
       " 'Beams',\n",
       " 'Bear',\n",
       " 'Bears',\n",
       " 'Beat',\n",
       " 'Because',\n",
       " 'Becket',\n",
       " 'Bedford',\n",
       " 'Beelzebub',\n",
       " 'Befooled',\n",
       " 'Before',\n",
       " 'Begone',\n",
       " 'Behold',\n",
       " 'Behring',\n",
       " 'Being',\n",
       " 'Belated',\n",
       " 'Belial',\n",
       " 'Believe',\n",
       " 'Belisarius',\n",
       " 'Bell',\n",
       " 'Bellies',\n",
       " 'Beloved',\n",
       " 'Below',\n",
       " 'Belshazzar',\n",
       " 'Belubed',\n",
       " 'Bench',\n",
       " 'Bendigoes',\n",
       " 'Beneath',\n",
       " 'Bengal',\n",
       " 'Benjamin',\n",
       " 'Bennett',\n",
       " 'Bentham',\n",
       " 'Berkshire',\n",
       " 'Berlin',\n",
       " 'Bernard',\n",
       " 'Besides',\n",
       " 'Bess',\n",
       " 'Best',\n",
       " 'Bestow',\n",
       " 'Bethink',\n",
       " 'Better',\n",
       " 'Betty',\n",
       " 'Between',\n",
       " 'Beware',\n",
       " 'Beyond',\n",
       " 'Bible',\n",
       " 'Bibles',\n",
       " 'Bibliographical',\n",
       " 'Bildad',\n",
       " 'Biographical',\n",
       " 'Birmah',\n",
       " 'Bishop',\n",
       " 'Bite',\n",
       " 'Black',\n",
       " 'Blacksmith',\n",
       " 'Blackstone',\n",
       " 'Blanc',\n",
       " 'Blanche',\n",
       " 'Blanco',\n",
       " 'Blang',\n",
       " 'Blanket',\n",
       " 'Blast',\n",
       " 'Bless',\n",
       " 'Blind',\n",
       " 'Blinding',\n",
       " 'Blocksburg',\n",
       " 'Blood',\n",
       " 'Bloody',\n",
       " 'Blue',\n",
       " 'Boat',\n",
       " 'Boats',\n",
       " 'Bobbing',\n",
       " 'Bolivia',\n",
       " 'Bombay',\n",
       " 'Bonapartes',\n",
       " 'Bone',\n",
       " 'Bones',\n",
       " 'Bonneterre',\n",
       " 'Booble',\n",
       " 'Book',\n",
       " 'Boomer',\n",
       " 'Boone',\n",
       " 'Bordeaux',\n",
       " 'Borean',\n",
       " 'Born',\n",
       " 'Borneo',\n",
       " 'Bosom',\n",
       " 'Boston',\n",
       " 'Both',\n",
       " 'Bottle',\n",
       " 'Bottom',\n",
       " 'Bourbons',\n",
       " 'Bout',\n",
       " 'Bouton',\n",
       " 'Bowditch',\n",
       " 'Bower',\n",
       " 'Boy',\n",
       " 'Boys',\n",
       " 'Brace',\n",
       " 'Brahma',\n",
       " 'Brahmins',\n",
       " 'Brandreth',\n",
       " 'Brazil',\n",
       " 'Breakfast',\n",
       " 'Bremen',\n",
       " 'Bress',\n",
       " 'Bridge',\n",
       " 'Brighggians',\n",
       " 'Bright',\n",
       " 'Bring',\n",
       " 'Brisson',\n",
       " 'Brit',\n",
       " 'Britain',\n",
       " 'British',\n",
       " 'Britons',\n",
       " 'Broad',\n",
       " 'Broadway',\n",
       " 'Broke',\n",
       " 'Brother',\n",
       " 'Browne',\n",
       " 'Brute',\n",
       " 'Buckets',\n",
       " 'Bud',\n",
       " 'Buffalo',\n",
       " 'Bulkington',\n",
       " 'Bull',\n",
       " 'Bulwarks',\n",
       " 'Bunger',\n",
       " 'Bungle',\n",
       " 'Bunyan',\n",
       " 'Buoy',\n",
       " 'Buoyed',\n",
       " 'Burke',\n",
       " 'Burkes',\n",
       " 'Burst',\n",
       " 'Burton',\n",
       " 'Burtons',\n",
       " 'Business',\n",
       " 'But',\n",
       " 'Butchers',\n",
       " 'Butler',\n",
       " 'By',\n",
       " 'Byward',\n",
       " 'C',\n",
       " 'Cabaco',\n",
       " 'Cabin',\n",
       " 'Cachalot',\n",
       " 'Cadiz',\n",
       " 'Caesar',\n",
       " 'Caesarian',\n",
       " 'Cain',\n",
       " 'Calais',\n",
       " 'Californian',\n",
       " 'Call',\n",
       " 'Callao',\n",
       " 'Cambyses',\n",
       " 'Camel',\n",
       " 'Campagna',\n",
       " 'Can',\n",
       " 'Canaan',\n",
       " 'Canada',\n",
       " 'Canadian',\n",
       " 'Canal',\n",
       " 'Canaller',\n",
       " 'Canallers',\n",
       " 'Canals',\n",
       " 'Canaris',\n",
       " 'Cancer',\n",
       " 'Candles',\n",
       " 'Cannibal',\n",
       " 'Cannibals',\n",
       " 'Cannon',\n",
       " 'Canst',\n",
       " 'Cant',\n",
       " 'Canterbury',\n",
       " 'Cap',\n",
       " 'Cape',\n",
       " 'Capes',\n",
       " 'Capricornus',\n",
       " 'Captain',\n",
       " 'Captains',\n",
       " 'Capting',\n",
       " 'Caramba',\n",
       " 'Careful',\n",
       " 'Carefully',\n",
       " 'Carey',\n",
       " 'Carpenter',\n",
       " 'Carpet',\n",
       " 'Carrol',\n",
       " 'Carson',\n",
       " 'Carthage',\n",
       " 'Caryatid',\n",
       " 'Case',\n",
       " 'Cash',\n",
       " 'Cassock',\n",
       " 'Castaway',\n",
       " 'Castle',\n",
       " 'Categut',\n",
       " 'Cathedral',\n",
       " 'Catholic',\n",
       " 'Cato',\n",
       " 'Catskill',\n",
       " 'Cattegat',\n",
       " 'Caught',\n",
       " 'Cave',\n",
       " 'Caw',\n",
       " 'Cellini',\n",
       " 'Central',\n",
       " 'Certain',\n",
       " 'Certainly',\n",
       " 'Cervantes',\n",
       " 'Cetacea',\n",
       " 'Cetacean',\n",
       " 'Cetology',\n",
       " 'Cetus',\n",
       " 'Ceylon',\n",
       " 'Chace',\n",
       " 'Chaldee',\n",
       " 'Champagne',\n",
       " 'Champollion',\n",
       " 'Channel',\n",
       " 'Chapel',\n",
       " 'Charing',\n",
       " 'Charity',\n",
       " 'Charlemagne',\n",
       " 'Charley',\n",
       " 'Chart',\n",
       " 'Chartering',\n",
       " 'Chase',\n",
       " 'Cheever',\n",
       " 'Cherries',\n",
       " 'Chestnut',\n",
       " 'Chief',\n",
       " 'Childe',\n",
       " 'Chili',\n",
       " 'Chilian',\n",
       " 'China',\n",
       " 'Chinese',\n",
       " 'Cholo',\n",
       " 'Chowder',\n",
       " 'Christ',\n",
       " 'Christendom',\n",
       " 'Christian',\n",
       " 'Christianity',\n",
       " 'Christians',\n",
       " 'Christmas',\n",
       " 'Church',\n",
       " 'Cinque',\n",
       " 'Circassian',\n",
       " 'Circumambulate',\n",
       " 'Cistern',\n",
       " 'Civitas',\n",
       " 'Clam',\n",
       " 'Clap',\n",
       " 'Claus',\n",
       " 'Clay',\n",
       " 'Clear',\n",
       " 'Clearing',\n",
       " 'Cleopatra',\n",
       " 'Cleveland',\n",
       " 'Clifford',\n",
       " 'Clinging',\n",
       " 'Clootz',\n",
       " 'Close',\n",
       " 'Closing',\n",
       " 'Cloud',\n",
       " 'Cluny',\n",
       " 'Coast',\n",
       " 'Cock',\n",
       " 'Cockatoo',\n",
       " 'Cod',\n",
       " 'Cods',\n",
       " 'Coenties',\n",
       " 'Coffin',\n",
       " 'Coffins',\n",
       " 'Cognac',\n",
       " 'Coke',\n",
       " 'Cold',\n",
       " 'Coleman',\n",
       " 'Coleridge',\n",
       " 'College',\n",
       " 'Colnett',\n",
       " 'Cologne',\n",
       " 'Colonies',\n",
       " 'Colossus',\n",
       " 'Columbus',\n",
       " 'Come',\n",
       " 'Coming',\n",
       " 'Commanded',\n",
       " 'Commanders',\n",
       " 'Commend',\n",
       " 'Commodore',\n",
       " 'Commodores',\n",
       " 'Common',\n",
       " 'Commonly',\n",
       " 'Commons',\n",
       " 'Commonwealth',\n",
       " 'Companies',\n",
       " 'Comparing',\n",
       " 'Concerning',\n",
       " 'Congo',\n",
       " 'Congregation',\n",
       " 'Congregational',\n",
       " 'Conjuror',\n",
       " 'Connecticut',\n",
       " 'Consequently',\n",
       " 'Consider',\n",
       " 'Considering',\n",
       " 'Constable',\n",
       " 'Constantine',\n",
       " 'Constantinople',\n",
       " 'Consumptive',\n",
       " 'Continents',\n",
       " 'Contrasted',\n",
       " 'Conversation',\n",
       " 'Convulsively',\n",
       " 'Cook',\n",
       " 'Cooke',\n",
       " 'Cooks',\n",
       " 'Cooper',\n",
       " 'Coopman',\n",
       " 'Copenhagen',\n",
       " 'Coppered',\n",
       " 'Corinthians',\n",
       " 'Corkscrew',\n",
       " 'Corlaer',\n",
       " 'Corlears',\n",
       " 'Coronation',\n",
       " 'Corresponding',\n",
       " 'Corrupt',\n",
       " 'Cough',\n",
       " 'Could',\n",
       " 'Count',\n",
       " 'Counterpane',\n",
       " 'County',\n",
       " 'Court',\n",
       " 'Cousin',\n",
       " 'Cowper',\n",
       " 'Crab',\n",
       " 'Crack',\n",
       " 'Crammer',\n",
       " 'Crappo',\n",
       " 'Crappoes',\n",
       " 'Crazed',\n",
       " 'Creagh',\n",
       " 'Created',\n",
       " 'Cretan',\n",
       " 'Crete',\n",
       " 'Crew',\n",
       " 'Crish',\n",
       " 'Crockett',\n",
       " 'Cross',\n",
       " 'Crossed',\n",
       " 'Crossing',\n",
       " 'Crotch',\n",
       " 'Crowding',\n",
       " 'Crown',\n",
       " 'Crozetts',\n",
       " 'Cruelty',\n",
       " 'Cruising',\n",
       " 'Cruppered',\n",
       " 'Crusaders',\n",
       " 'Crushed',\n",
       " 'Crying',\n",
       " 'Cuba',\n",
       " 'Curious',\n",
       " 'Curse',\n",
       " 'Cursed',\n",
       " 'Curses',\n",
       " 'Cussed',\n",
       " 'Customs',\n",
       " 'Cut',\n",
       " 'Cutter',\n",
       " 'Cutting',\n",
       " 'Cuvier',\n",
       " 'Cyclades',\n",
       " 'Czar',\n",
       " 'D',\n",
       " 'Daboll',\n",
       " 'Daggoo',\n",
       " 'Dagon',\n",
       " 'Dame',\n",
       " 'Damn',\n",
       " 'Damocles',\n",
       " 'Dampier',\n",
       " 'Dan',\n",
       " 'Dance',\n",
       " 'Danes',\n",
       " 'Daniel',\n",
       " 'Danish',\n",
       " 'Dante',\n",
       " 'Dantean',\n",
       " 'Dar',\n",
       " 'Dardanelles',\n",
       " 'Darien',\n",
       " 'Darkness',\n",
       " 'Darmonodes',\n",
       " 'Dart',\n",
       " 'Dash',\n",
       " 'Dashing',\n",
       " 'Dauphine',\n",
       " 'Davis',\n",
       " 'Davy',\n",
       " 'Day',\n",
       " 'Days',\n",
       " 'De',\n",
       " 'Deacon',\n",
       " 'Dead',\n",
       " 'Death',\n",
       " 'Decanter',\n",
       " 'Decapitation',\n",
       " 'December',\n",
       " 'Deck',\n",
       " 'Deep',\n",
       " 'Deer',\n",
       " 'Deity',\n",
       " 'Del',\n",
       " 'Deliberately',\n",
       " 'Delight',\n",
       " 'Delightful',\n",
       " 'Deliverer',\n",
       " 'Delta',\n",
       " 'Den',\n",
       " 'Denderah',\n",
       " 'Depend',\n",
       " 'Derick',\n",
       " 'Dericks',\n",
       " 'Descartian',\n",
       " 'Descending',\n",
       " 'Desecrated',\n",
       " 'Desmarest',\n",
       " 'Desolation',\n",
       " 'Despairing',\n",
       " 'Despatch',\n",
       " 'Detached',\n",
       " 'Deuteronomy',\n",
       " 'Devil',\n",
       " 'Devils',\n",
       " 'Dey',\n",
       " 'Diaz',\n",
       " 'Dick',\n",
       " 'Did',\n",
       " 'Didn',\n",
       " 'Didst',\n",
       " 'Diminish',\n",
       " 'Ding',\n",
       " 'Dinner',\n",
       " 'Dinting',\n",
       " 'Discovery',\n",
       " 'Disdain',\n",
       " 'Dish',\n",
       " 'Dismal',\n",
       " 'Dissect',\n",
       " 'Dives',\n",
       " 'Divine',\n",
       " 'Diving',\n",
       " 'Do',\n",
       " 'Doctor',\n",
       " 'Dodge',\n",
       " 'Does',\n",
       " 'Doesn',\n",
       " 'Dog',\n",
       " 'Dolly',\n",
       " 'Dome',\n",
       " 'Dominic',\n",
       " 'Don',\n",
       " 'Dons',\n",
       " 'Doom',\n",
       " 'Dorchester',\n",
       " 'Dost',\n",
       " 'Doubloon',\n",
       " 'Doubtless',\n",
       " 'Doubts',\n",
       " 'Dough',\n",
       " 'Dover',\n",
       " 'Down',\n",
       " 'Dr',\n",
       " 'Dragged',\n",
       " 'Dragon',\n",
       " 'Drat',\n",
       " 'Drawing',\n",
       " 'Drawn',\n",
       " 'Draws',\n",
       " 'Drink',\n",
       " 'Drinking',\n",
       " 'Drive',\n",
       " 'Drop',\n",
       " 'Dropping',\n",
       " 'Dry',\n",
       " 'Duck',\n",
       " 'Dugongs',\n",
       " 'Duke',\n",
       " 'Dunder',\n",
       " 'Dunfermline',\n",
       " 'Dunkirk',\n",
       " 'Duodecimo',\n",
       " 'Duodecimoes',\n",
       " 'Durand',\n",
       " 'Durer',\n",
       " 'During',\n",
       " 'Dusk',\n",
       " 'Dut',\n",
       " 'Dutch',\n",
       " 'Dutchman',\n",
       " 'Dying',\n",
       " 'E',\n",
       " 'Each',\n",
       " 'Eagle',\n",
       " 'Earl',\n",
       " 'Earls',\n",
       " 'Earthsman',\n",
       " 'East',\n",
       " 'Eastern',\n",
       " 'Easy',\n",
       " 'Ebony',\n",
       " 'Ecclesiastes',\n",
       " 'Eckerman',\n",
       " 'Eddystone',\n",
       " 'Edgewise',\n",
       " 'Edmund',\n",
       " 'Edward',\n",
       " 'Ego',\n",
       " 'Egypt',\n",
       " 'Egyptian',\n",
       " 'Egyptians',\n",
       " 'Eh',\n",
       " 'Ehrenbreitstein',\n",
       " 'Eight',\n",
       " 'Either',\n",
       " 'Elbe',\n",
       " 'Electors',\n",
       " 'Elephant',\n",
       " 'Elephanta',\n",
       " 'Elephants',\n",
       " 'Elijah',\n",
       " 'Ellenborough',\n",
       " 'Elsewhere',\n",
       " 'Emblazonings',\n",
       " 'Emboldened',\n",
       " 'Emir',\n",
       " 'Emperor',\n",
       " 'Emperors',\n",
       " 'Empire',\n",
       " 'End',\n",
       " 'Enderbies',\n",
       " 'Enderby',\n",
       " 'Enderbys',\n",
       " 'England',\n",
       " 'Englander',\n",
       " 'English',\n",
       " 'Englishman',\n",
       " 'Englishmen',\n",
       " 'Enough',\n",
       " 'Enter',\n",
       " 'Entering',\n",
       " 'Entreaties',\n",
       " 'Enveloped',\n",
       " 'Ephesian',\n",
       " 'Epilogue',\n",
       " 'Epitome',\n",
       " 'Equality',\n",
       " 'Equator',\n",
       " 'Equatorial',\n",
       " 'Ere',\n",
       " 'Erie',\n",
       " 'Erromanggoans',\n",
       " 'Erroneous',\n",
       " 'Erskine',\n",
       " 'Esau',\n",
       " 'Espied',\n",
       " 'Espying',\n",
       " 'Esquimaux',\n",
       " 'Essex',\n",
       " 'Et',\n",
       " 'Eternities',\n",
       " 'Eternity',\n",
       " 'Ethiopian',\n",
       " 'Euclid',\n",
       " 'Euclidean',\n",
       " 'Euroclydon',\n",
       " 'Europa',\n",
       " 'Europe',\n",
       " 'European',\n",
       " 'Evangelist',\n",
       " 'Evangelists',\n",
       " 'Even',\n",
       " 'Ever',\n",
       " 'Every',\n",
       " 'Evil',\n",
       " 'Ex',\n",
       " 'Excellent',\n",
       " 'Excepting',\n",
       " 'Exception',\n",
       " 'Excuse',\n",
       " 'Expedition',\n",
       " 'Expeditions',\n",
       " 'Explain',\n",
       " 'Exploring',\n",
       " 'Extending',\n",
       " 'Ezekiel',\n",
       " 'F',\n",
       " 'Fa',\n",
       " 'Face',\n",
       " 'Fain',\n",
       " 'Faintly',\n",
       " 'Fair',\n",
       " 'Faith',\n",
       " 'Falsehood',\n",
       " 'Fanning',\n",
       " 'Far',\n",
       " 'Farewell',\n",
       " 'Fashioned',\n",
       " 'Fast',\n",
       " 'Fasting',\n",
       " 'Fat',\n",
       " 'Fata',\n",
       " 'Fate',\n",
       " 'Fates',\n",
       " 'Father',\n",
       " 'Fe',\n",
       " 'Fear',\n",
       " 'Fearing',\n",
       " 'February',\n",
       " 'Fedallah',\n",
       " 'Feegee',\n",
       " 'Feegeeans',\n",
       " 'Feegees',\n",
       " 'Feel',\n",
       " 'Feet',\n",
       " 'Fejee',\n",
       " 'Fellow',\n",
       " 'Ferdinando',\n",
       " 'Fernandes',\n",
       " 'Fetch',\n",
       " 'Few',\n",
       " 'Fields',\n",
       " 'Fiercely',\n",
       " 'Fiery',\n",
       " 'Fife',\n",
       " 'Fifth',\n",
       " 'Figuera',\n",
       " 'Fill',\n",
       " 'Fin',\n",
       " 'Finally',\n",
       " 'Find',\n",
       " 'Finding',\n",
       " 'Fine',\n",
       " 'Fired',\n",
       " 'First',\n",
       " 'Fish',\n",
       " 'Fisheries',\n",
       " 'Fishery',\n",
       " 'Fishes',\n",
       " 'Fishiest',\n",
       " 'Fits',\n",
       " 'Fitz',\n",
       " 'Five',\n",
       " 'Flask',\n",
       " 'Flat',\n",
       " 'Fleece',\n",
       " 'Fleet',\n",
       " 'Flip',\n",
       " 'Floating',\n",
       " 'Floundered',\n",
       " 'Flounders',\n",
       " 'Flukes',\n",
       " 'Flying',\n",
       " 'Fogo',\n",
       " 'Folding',\n",
       " 'Folger',\n",
       " 'Folgers',\n",
       " 'Folio',\n",
       " 'Folios',\n",
       " 'Fool',\n",
       " 'Foolish',\n",
       " 'For',\n",
       " 'Forced',\n",
       " 'Fore',\n",
       " 'Forecastle',\n",
       " 'Forehead',\n",
       " 'Foremost',\n",
       " 'Forge',\n",
       " 'Form',\n",
       " 'Forming',\n",
       " 'Formosa',\n",
       " 'Forthwith',\n",
       " 'Forty',\n",
       " 'Forward',\n",
       " 'Fossil',\n",
       " 'Fountain',\n",
       " 'Fourth',\n",
       " 'France',\n",
       " 'Frankfort',\n",
       " 'Franklin',\n",
       " 'Frederick',\n",
       " 'Free',\n",
       " 'Freely',\n",
       " 'Freeze',\n",
       " 'French',\n",
       " 'Frenchman',\n",
       " 'Frenchmen',\n",
       " 'Friar',\n",
       " 'Friend',\n",
       " 'Friends',\n",
       " 'Friesland',\n",
       " 'Frighted',\n",
       " 'Frobisher',\n",
       " 'Froissart',\n",
       " 'From',\n",
       " 'Fuego',\n",
       " 'Full',\n",
       " 'Funeral',\n",
       " 'Furl',\n",
       " 'Further',\n",
       " 'Furthermore',\n",
       " 'Future',\n",
       " 'Gabriel',\n",
       " 'Gaining',\n",
       " 'Gall',\n",
       " 'Galleries',\n",
       " 'Gallipagos',\n",
       " 'Gam',\n",
       " 'Gamming',\n",
       " 'Ganders',\n",
       " 'Ganges',\n",
       " 'Gardiner',\n",
       " 'Garnery',\n",
       " 'Gases',\n",
       " 'Gate',\n",
       " 'Gather',\n",
       " 'Gay',\n",
       " 'Gayer',\n",
       " 'Gayhead',\n",
       " 'Gazette',\n",
       " 'Gemini',\n",
       " 'General',\n",
       " 'Genesis',\n",
       " 'Geneva',\n",
       " 'Genius',\n",
       " 'Gentlemen',\n",
       " 'Gently',\n",
       " 'Geological',\n",
       " 'George',\n",
       " 'Ger',\n",
       " 'Germain',\n",
       " 'German',\n",
       " 'Germans',\n",
       " 'Gesner',\n",
       " 'Get',\n",
       " 'Ghent',\n",
       " 'Gibraltar',\n",
       " 'Gifted',\n",
       " 'Gilder',\n",
       " 'Ginger',\n",
       " 'Give',\n",
       " 'Giver',\n",
       " 'Giving',\n",
       " 'Glacier',\n",
       " 'Glancing',\n",
       " 'Glen',\n",
       " 'Gliding',\n",
       " 'Glimpses',\n",
       " 'Globe',\n",
       " 'Glory',\n",
       " 'Gnawed',\n",
       " 'Go',\n",
       " 'Goa',\n",
       " 'Goat',\n",
       " 'God',\n",
       " 'Gods',\n",
       " ...]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get all the words which their first letter is capitalized:\n",
    "sorted(item for item in set(moby_dick_book1) if item.istitle())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8YWHbsQyIhC4"
   },
   "source": [
    "And there are more. if `wrd` is a string, then, for example:\n",
    "\n",
    "* `wrd.islower()` will return true if the word is all lowercase\n",
    "* `wrd.isalpha()` will return true if all the character in the string are letters\n",
    "\n",
    "and there are also: `wrd.startswith('str')`, `wrd.isdigit()`, `wr.isalnum()`\n",
    "and [many others](https://www.w3schools.com/python/python_ref_string.asp)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jl2mxc9xa_nC"
   },
   "source": [
    "## Exercise #4: Functions and substrings search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "id": "wiP7qcUTIUS6"
   },
   "outputs": [],
   "source": [
    "from typing import List\n",
    "\n",
    "### Exercise:\n",
    "\n",
    "def detect_string(tokens: List[str], search_str: str, search_position: int = 0) -> List[str]:\n",
    "    search_str = search_str.lower()\n",
    "\n",
    "    # defining the filtering logic based on search_position with lowercase comparison\n",
    "    if search_position == 0:  \n",
    "        result = {token.lower() for token in tokens if search_str in token.lower() and token.isalpha()}\n",
    "    elif search_position == 1:  \n",
    "        result = {token.lower() for token in tokens if token.lower().startswith(search_str) and token.isalpha()}\n",
    "    elif search_position == 2:  \n",
    "        result = {token.lower() for token in tokens if token.lower().endswith(search_str) and token.isalpha()}\n",
    "\n",
    "    return sorted(result)\n",
    "# debugged code\n",
    "#output = detect_string(text1, 'larg')\n",
    "#print(\"Function Output:\", output)\n",
    "\n",
    "# checking  if output matches expected result\n",
    "#expected_output = ['enlarge',\n",
    " #'enlarged',\n",
    " #'enlarges',\n",
    " #'large',\n",
    " #'largely',\n",
    " #'largeness',\n",
    " #'larger',\n",
    " #'largest']\n",
    "#print(\"Expected Output:\", expected_output)\n",
    "\n",
    "\n",
    "  #\"\"\"Returns a sorted list of the vocabulary tokens which match the search conditions\n",
    "\n",
    "  #params:\n",
    "   # tokens: a document tokens list.\n",
    "   # search_str: a string to search in the token list\n",
    "   # search_position: one of the following:\n",
    "    #  0 - anywhere in the string\n",
    "     # 1 - searches for the string at the beginning of the token\n",
    "     # 2 - searches for the string at the end of the token\n",
    "  #\"\"\"\n",
    "  ### Fill in this function to returns the result of searching for the\n",
    "  ### given string \"search_str\" in the token vocabulary \"tokens\", according to\n",
    "  ### the position parameter, as explained in the docstring\n",
    "\n",
    "\n",
    "\n",
    "###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "id": "6_t95CzIL27v"
   },
   "outputs": [],
   "source": [
    "### Test:\n",
    "assert detect_string(moby_dick_book1, 'tably', 2) == ['comfortably',\n",
    " 'discreditably',\n",
    " 'illimitably',\n",
    " 'immutably',\n",
    " 'indubitably',\n",
    " 'inevitably',\n",
    " 'inscrutably',\n",
    " 'profitably',\n",
    " 'unaccountably',\n",
    " 'unwarrantably']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "id": "nrBjzeHIL4_V"
   },
   "outputs": [],
   "source": [
    "### Test:\n",
    "assert detect_string(moby_dick_book1, 'argu', 1) == ['argue', 'argued', 'arguing', 'argument', 'arguments']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "id": "HOJPKwpnL6nQ"
   },
   "outputs": [],
   "source": [
    "### Test:\n",
    "assert detect_string(moby_dick_book1, 'arg', 2) == []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "id": "n8V2lvmWL8TC"
   },
   "outputs": [],
   "source": [
    "### Test\n",
    "assert detect_string(moby_dick_book1, 'larg') == ['enlarge',\n",
    " 'enlarged',\n",
    " 'enlarges',\n",
    " 'large',\n",
    " 'largely',\n",
    " 'largeness',\n",
    " 'larger',\n",
    " 'largest']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "s8QYjforL9vI"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "1OqX9GeN216c"
   ],
   "include_colab_link": true,
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
